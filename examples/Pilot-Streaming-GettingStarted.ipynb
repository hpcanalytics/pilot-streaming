{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Getting Started with Pilot-Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-28T17:59:21.386383Z",
     "start_time": "2017-12-28T17:59:21.364643Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(\"..\")\n",
    "from pilot.streaming import PilotComputeService"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-28T18:00:04.950564Z",
     "start_time": "2017-12-28T17:59:22.095228Z"
    },
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/q224516/anaconda2/lib/python2.7/site-packages/radical/utils/atfork/stdlib_fixer.py:72: UserWarning: logging handlers already registered.\n",
      "  warnings.warn('logging handlers already registered.')\n",
      "INFO:root:Loading SAGA-Hadoop version: 0.31.2 on LW10617975\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: failed to run bootstrap: (127)(/bin/sh: .saga/adaptors/shell_job//wrapper.sh: No such file or directory\n",
      ") (/Users/q224516/anaconda2/lib/python2.7/site-packages/saga/adaptors/shell/shell_job.py +632 (initialize)  :  raise saga.NoSuccess (\"failed to run bootstrap: (%s)(%s)\" % (ret, out)))\n",
      "/tmp/work/spark-2.2.1-bin-hadoop2.7/conf/masters\n",
      "Looking for /tmp/work/spark-2.2.1-bin-hadoop2.7/conf/masters\n",
      "Looking for /tmp/work/spark-2.2.1-bin-hadoop2.7/conf/masters\n",
      "Looking for /tmp/work/spark-2.2.1-bin-hadoop2.7/conf/masters\n",
      "Looking for /tmp/work/spark-2.2.1-bin-hadoop2.7/conf/masters\n",
      "Looking for /tmp/work/spark-2.2.1-bin-hadoop2.7/conf/masters\n",
      "Looking for /tmp/work/spark-2.2.1-bin-hadoop2.7/conf/masters\n",
      "Looking for /tmp/work/spark-2.2.1-bin-hadoop2.7/conf/masters\n",
      "Looking for /tmp/work/spark-2.2.1-bin-hadoop2.7/conf/masters\n",
      "Looking for /tmp/work/spark-2.2.1-bin-hadoop2.7/conf/masters\n",
      "Looking for /tmp/work/spark-2.2.1-bin-hadoop2.7/conf/masters\n",
      "Looking for /tmp/work/spark-2.2.1-bin-hadoop2.7/conf/masters\n",
      "Looking for /tmp/work/spark-2.2.1-bin-hadoop2.7/conf/masters\n",
      "Looking for /tmp/work/spark-2.2.1-bin-hadoop2.7/conf/masters\n",
      "Looking for /tmp/work/spark-2.2.1-bin-hadoop2.7/conf/masters\n",
      "Looking for /tmp/work/spark-2.2.1-bin-hadoop2.7/conf/masters\n",
      "Looking for /tmp/work/spark-2.2.1-bin-hadoop2.7/conf/masters\n",
      "Looking for /tmp/work/spark-2.2.1-bin-hadoop2.7/conf/masters\n",
      "Looking for /tmp/work/spark-2.2.1-bin-hadoop2.7/conf/masters\n",
      "Looking for /tmp/work/spark-2.2.1-bin-hadoop2.7/conf/masters\n",
      "Looking for /tmp/work/spark-2.2.1-bin-hadoop2.7/conf/masters\n",
      "Looking for /tmp/work/spark-2.2.1-bin-hadoop2.7/conf/masters\n",
      "Looking for /tmp/work/spark-2.2.1-bin-hadoop2.7/conf/masters\n",
      "Looking for /tmp/work/spark-2.2.1-bin-hadoop2.7/conf/masters\n",
      "Looking for /tmp/work/spark-2.2.1-bin-hadoop2.7/conf/masters\n",
      "Looking for /tmp/work/spark-2.2.1-bin-hadoop2.7/conf/masters\n",
      "Looking for /tmp/work/spark-2.2.1-bin-hadoop2.7/conf/masters\n",
      "Looking for /tmp/work/spark-2.2.1-bin-hadoop2.7/conf/masters\n",
      "Looking for /tmp/work/spark-2.2.1-bin-hadoop2.7/conf/masters\n",
      "Looking for /tmp/work/spark-2.2.1-bin-hadoop2.7/conf/masters\n",
      "Looking for /tmp/work/spark-2.2.1-bin-hadoop2.7/conf/masters\n",
      "Looking for /tmp/work/spark-2.2.1-bin-hadoop2.7/conf/masters\n",
      "Looking for /tmp/work/spark-2.2.1-bin-hadoop2.7/conf/masters\n",
      "Looking for /tmp/work/spark-2.2.1-bin-hadoop2.7/conf/masters\n",
      "Looking for /tmp/work/spark-2.2.1-bin-hadoop2.7/conf/masters\n",
      "Looking for /tmp/work/spark-2.2.1-bin-hadoop2.7/conf/masters\n",
      "Looking for /tmp/work/spark-2.2.1-bin-hadoop2.7/conf/masters\n",
      "Looking for /tmp/work/spark-2.2.1-bin-hadoop2.7/conf/masters\n",
      "Looking for /tmp/work/spark-2.2.1-bin-hadoop2.7/conf/masters\n",
      "Looking for /tmp/work/spark-2.2.1-bin-hadoop2.7/conf/masters\n",
      "Looking for /tmp/work/spark-2.2.1-bin-hadoop2.7/conf/masters\n",
      "Looking for /tmp/work/spark-2.2.1-bin-hadoop2.7/conf/masters\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-739559290244>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;34m\"type\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"spark\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m }\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mpilot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPilotComputeService\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_pilot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpilot_compute_description\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpilot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_details\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/q224516/workspace-saga/bigjob/pilot-streaming/pilot/streaming.py\u001b[0m in \u001b[0;36mcreate_pilot\u001b[0;34m(cls, pilotcompute_description)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mpilot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m             \u001b[0mpilot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__start_spark_cluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpilotcompute_description\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mpilot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/q224516/workspace-saga/bigjob/pilot-streaming/pilot/streaming.py\u001b[0m in \u001b[0;36m__start_spark_cluster\u001b[0;34m(self, pilotcompute_description)\u001b[0m\n\u001b[1;32m    279\u001b[0m         )\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0mdetails\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPilotComputeService\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_spark_config_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworking_directory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m         \u001b[0mpilot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPilotCompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaga_job\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpilot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/q224516/workspace-saga/bigjob/pilot-streaming/pilot/streaming.py\u001b[0m in \u001b[0;36mget_spark_config_data\u001b[0;34m(cls, working_directory)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaster_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFalse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcounter\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m600\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Looking for %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmaster_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m             \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcounter\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "os.environ[\"SAGA_VERBOSE\"]=\"100\"\n",
    "pilot_compute_description = {\n",
    "    \"service_url\":\"ssh://localhost\",\n",
    "    \"number_cores\": 1,\n",
    "    \"cores_per_node\":1,\n",
    "    \"type\":\"spark\"\n",
    "}\n",
    "pilot = PilotComputeService.create_pilot(pilot_compute_description)\n",
    "\n",
    "print str(pilot.get_details())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-28T17:51:16.198166Z",
     "start_time": "2017-12-28T17:51:16.186753Z"
    },
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-12-28T17:51:55.430862Z",
     "start_time": "2017-12-28T17:51:55.093479Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:py4j.java_gateway:Command to send: c\n",
      "o10\n",
      "sc\n",
      "e\n",
      "\n",
      "DEBUG:py4j.java_gateway:Answer received: !yro18\n",
      "DEBUG:py4j.java_gateway:Command to send: c\n",
      "o18\n",
      "defaultParallelism\n",
      "e\n",
      "\n",
      "DEBUG:py4j.java_gateway:Answer received: !yi8\n",
      "DEBUG:py4j.java_gateway:Command to send: r\n",
      "u\n",
      "PythonRDD\n",
      "rj\n",
      "e\n",
      "\n",
      "DEBUG:py4j.java_gateway:Answer received: !ycorg.apache.spark.api.python.PythonRDD\n",
      "DEBUG:py4j.java_gateway:Command to send: r\n",
      "m\n",
      "org.apache.spark.api.python.PythonRDD\n",
      "readRDDFromFile\n",
      "e\n",
      "\n",
      "DEBUG:py4j.java_gateway:Answer received: !ym\n",
      "DEBUG:py4j.java_gateway:Command to send: c\n",
      "z:org.apache.spark.api.python.PythonRDD\n",
      "readRDDFromFile\n",
      "ro10\n",
      "s/private/var/folders/f1/13wgdp7n01sg06llx4hcsv9nr44gjx/T/spark-ccfb5b8c-c30a-4563-9953-adc164f5e83b/pyspark-c9894cc1-c817-4e20-8caf-b7174af44067/tmpX0eApl\n",
      "i8\n",
      "e\n",
      "\n",
      "DEBUG:py4j.java_gateway:Answer received: !yro19\n",
      "DEBUG:py4j.java_gateway:Command to send: c\n",
      "o19\n",
      "id\n",
      "e\n",
      "\n",
      "DEBUG:py4j.java_gateway:Answer received: !yi0\n",
      "DEBUG:py4j.java_gateway:Command to send: c\n",
      "o19\n",
      "toString\n",
      "e\n",
      "\n",
      "DEBUG:py4j.java_gateway:Answer received: !ysParallelCollectionRDD[0] at parallelize at PythonRDD.scala:480\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:480"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_threshold": 6,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
